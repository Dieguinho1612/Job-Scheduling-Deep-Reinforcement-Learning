{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890384ac",
   "metadata": {},
   "source": [
    "# Scheduling Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13dba49",
   "metadata": {},
   "source": [
    "## Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951b2e2",
   "metadata": {},
   "source": [
    "In this Notebook we will implement the function of how the Neural Networks takes an action given a state-data-input.<br>\n",
    "From there, we will develop schedules according to the policy it induces.<br>\n",
    "\n",
    "With this, we can estimate data of higher Job numbers, by taking a Job Scheduling Problem with <i>n</i> Jobs, creating the successor state for every possible action and estimate the optimal future costs from there on with the Neural Network that has been trained on <i>n-1</i> Jobs already. By repeating this process, we can arbitrarily increase the number of Jobs on which we train our model.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38aacc",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d81728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import import_ipynb\n",
    "from Jobs_and_Machines import *\n",
    "import States_and_Policies\n",
    "from States_and_Policies import *\n",
    "import Data_for_NN\n",
    "from Data_for_NN import *\n",
    "from Action_Pointer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41390ff8",
   "metadata": {},
   "source": [
    "### Random Job Scheduling Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c79b6",
   "metadata": {},
   "source": [
    "The following notebook defines the environmental conditions by the given global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f5be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Global_Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be68e25",
   "metadata": {},
   "source": [
    "We want to simulate random Job Scheduling Problems according to these environmental conditions. Since some of the sampled conditions may change in the number of Jobs and Machines during this notebook, we need a function to update all these global variables within all the imported notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Random_Generator\n",
    "from Random_Generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e259a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate random Job Scheduling Problem and pass environmental parameters as global variables to other notebooks\n",
    "def create_JS_environment():\n",
    "    \n",
    "    #these variables define the problem environment and get randomly reassigned for every Job Scheduling Problem\n",
    "    global max_runtime, max_init_runtime, list_jobs, list_machines\n",
    "    environment = generate_random_environment()\n",
    "    max_runtime, max_init_runtime, list_jobs, list_machines = environment\n",
    "    #create dictionary of all environmental variables of this Job Scheduling Problem\n",
    "    list_env = ['max_runtime', 'max_init_runtime', 'list_jobs', 'list_machines']\n",
    "    dict_env = dict((list_env[i],environment[i]) for i in range(len(environment)))\n",
    "    #pass them as global variables, so that imported notebooks can access them\n",
    "    Global_Variables.set_var_to_global(dict_env)\n",
    "    change_global_var_of_module(States_and_Policies)\n",
    "    change_global_var_of_module(Data_for_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c2624",
   "metadata": {},
   "source": [
    "Since we want to estimate data of Job Scheduling Problems with higher numbers of Machines and also use our Neural Network to create schedules to Job Scheduling Problems that consist of more Jobs and Machines to test how well it generalizes performancewise, we need a function to update the global variable of both, the numbers of Jobs and that of Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increase number of Jobs globally\n",
    "def increase_n(new_n):\n",
    "    global n\n",
    "    n = new_n\n",
    "    Global_Variables.n = n\n",
    "    Random_Generator.n = n\n",
    "    States_and_Policies.n = n\n",
    "    Data_for_NN.n = n\n",
    "    return n\n",
    "    \n",
    "#increase number of Machines globally\n",
    "def increase_m(new_m):\n",
    "    global m\n",
    "    m = new_m\n",
    "    Global_Variables.m = m\n",
    "    Random_Generator.m = m\n",
    "    States_and_Policies.m = m\n",
    "    Data_for_NN.m = m\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebdd508",
   "metadata": {},
   "source": [
    "### Schedule based on Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd8e2e4",
   "metadata": {},
   "source": [
    "We now want to create a rule of action and therefore scheduling based on the policy induced by the Neural Network.<br>\n",
    "First, we need to be able to load any previously trained version of our Network.<br>\n",
    "The user needs to update the path to whereever the Network is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load desired version of Neural Network\n",
    "def load_NN(NN_name,path):\n",
    "    \n",
    "    NN = keras.models.load_model(f'{path}/{NN_name}.h5', custom_objects={'FeedForward': FeedForward, 'Pointer': Pointer, 'MSE_with_Softmax': MSE_with_Softmax, 'costs':costs})\n",
    "    NN.run_eagerly = True\n",
    "    \n",
    "    return NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651af3f",
   "metadata": {},
   "source": [
    "Now, we need to create a function that defines how to act based on the outputs of the Neural Network to a Job Scheduling Problem. With other words, we define the policy it induces and therefore a network based scheduling rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31882e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to act in a given state according to Neural Network\n",
    "def act(NN, state):\n",
    "    \n",
    "    n_state = sum(state.jobs_remaining) #get job number of state\n",
    "    m_state = sum(state.machines_on_duty) #get machine number of state\n",
    "    \n",
    "    #object list of remaining Jobs\n",
    "    remaining_jobs = [job for job in list_jobs if state.jobs_remaining[job.index] == 1]\n",
    "    #append action of turning off machine\n",
    "    remaining_jobs.append(None)\n",
    "    #extract network compatible data from state\n",
    "    data = [np.expand_dims(state.input[0],axis=0), np.expand_dims(state.input[1],axis=0)]\n",
    "    #estimate Q-values for every action\n",
    "    act_values = NN.predict(data, verbose=0)\n",
    "    \n",
    "    #since outputs are normalized, action associated with maximum value is estimated to produce minimal costs\n",
    "    if m_state > 1:\n",
    "        action = np.argmax(act_values[0])\n",
    "    #cannot turn off last machine\n",
    "    else:\n",
    "        action = np.argmax(act_values[0][:-1])\n",
    "    #if recommended action is assigning a Job, we have to consider the permutation of jobs\n",
    "    if action < n_state:\n",
    "        action = state.permutation[0][action]\n",
    "    \n",
    "    #job associated with recommended action.\n",
    "    job = remaining_jobs[action] #is \"None\" if action==n_state corresponds to machine shutdown\n",
    "    \n",
    "    return action, job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d97b6b",
   "metadata": {},
   "source": [
    "Cost distributions might change towards the end of a schedule. At the same time, brute force computing the optimal policy \n",
    "for the last few decisions becomes very cheap then. So to give some relief to our Neural Network and therefore put its focus on learning more complex scheduling situations, we did not train it on states with less than 3 pending Jobs or only 1 Machine still working for not more than 8 remaining Jobs. We therefore will add the option of computing the optimal costs for these states with the following functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab48cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the set of all possible successor states from current state on\n",
    "def compute_remaining_states(current_state):\n",
    "        \n",
    "    #initiate list of states = []\n",
    "    list_states = []\n",
    "    \n",
    "    #list of current states\n",
    "    current_states = [current_state]\n",
    "    \n",
    "    #go through every current state, save their successors, add current states to list of all states\n",
    "        #then define successor states as current states, clear list of successor states and repeat until done\n",
    "    while current_states:\n",
    "        \n",
    "        #empty list of all successor states\n",
    "        successor_states = []\n",
    "        \n",
    "        #create and add all successor states of current states\n",
    "        for state in current_states:\n",
    "            \n",
    "            #list of all successors of this state\n",
    "            state_successors = []\n",
    "            \n",
    "            #check if state is not final yet\n",
    "            if sum(state.jobs_remaining) > 0:\n",
    "                \n",
    "                #create one state for every remaining job assigned to the free machine with lowest index\n",
    "                machine = list_machines[state.machine]\n",
    "                \n",
    "                #loop through jobs\n",
    "                list_jobs_remaining = [job for job in list_jobs if state.jobs_remaining[job.index] == 1]\n",
    "                for index, job in enumerate(list_jobs):\n",
    "                    #check if the job still has to be done\n",
    "                    if state.jobs_remaining[index] == 1:\n",
    "                        #assign job to machine\n",
    "                        state_successors.append(assign_job(state,job,machine))    \n",
    "                \n",
    "                #check if turning it off is an option\n",
    "                if sum(state.machines_on_duty) > 1:                       \n",
    "                    #add successor state created by shutting down machine\n",
    "                    state_successors.append(turn_off_machine(state, machine))\n",
    "                        \n",
    "                \n",
    "            #add successor list to the attributes of state\n",
    "            state.successors = state_successors\n",
    "            \n",
    "            #add successors of this state to the list of all successors of all current states\n",
    "            successor_states += state_successors\n",
    "        \n",
    "        #add current states to list of all states\n",
    "        list_states += current_states\n",
    "        \n",
    "        #the successor states then become the current states\n",
    "        current_states = successor_states\n",
    "        \n",
    "    return list_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal policy for remaining states\n",
    "def compute_remaining_policy(current_state):\n",
    "    \n",
    "    #compute all remaining possible successor states from current state on\n",
    "    remaining_states = create_all_states(from_state=current_state)\n",
    "    #calculate their true Q-values\n",
    "    backtracking(remaining_states)\n",
    "    #deduce the optimal policy with a greedy approach\n",
    "    remaining_policy = optimal_policy(remaining_states)\n",
    "    return (remaining_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04194da",
   "metadata": {},
   "source": [
    "We can now construct a policy for a given Job Scheduling Problem based our Neural Network and the policy it induces.<br>\n",
    "\n",
    "Among other aims, we want to train our Neural Network on estimated data sets of higher Job numbers. Since, however, our Neural Network that has already been trained on the ground-true Q-values of Job Scheduling Problems with 8 Jobs and 4 Machines, we assume these supervisedly learned weights to very likely be superior to any that we could achieve by applying reinforced learning techniques. Consequently, we will switch to them as soon as a Job Scheduling Problem has not more than 8 remaining Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4df13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(NN, Sup_Tar_NN, state, opt_end=True, n_min=3, m_min=1):\n",
    "    \n",
    "    state_list = []\n",
    "    action_list = []\n",
    "\n",
    "    \n",
    "    n_state = sum(state.jobs_remaining) #get job number of state\n",
    "    m_state = sum(state.machines_on_duty) #get machine number of state\n",
    "    \n",
    "    #minimum number of jobs and machines depends on whether optimal schedule shall be computed in the end or not\n",
    "    n_min = max(n_min*opt_end,1)\n",
    "    m_min = m_min*opt_end\n",
    "    \n",
    "    while n_state >= n_min and (m_state >= m_min or n_state > 8):\n",
    "        \n",
    "        #create data input for NN. Saved under state.input\n",
    "        if not state.input:\n",
    "            seq_data(state)\n",
    "        \n",
    "        #use uptrained target network when more than 8 Jobs remain\n",
    "        if n_state > 8:\n",
    "            action, job = act(NN, state)\n",
    "        #use supervised target network when 8 or less Jobs remain\n",
    "        else:\n",
    "            action, job = act(Sup_Tar_NN, state)\n",
    "        \n",
    "        #currently free machine as object\n",
    "        machine = list_machines[state.machine]\n",
    "        \n",
    "        #if job shall get assigned\n",
    "        if job:\n",
    "            next_state = assign_job(state, job, machine)\n",
    "        #if machine shall get turned off\n",
    "        else:\n",
    "            next_state = turn_off_machine(state, machine)\n",
    "        \n",
    "        action_list.append((action,state.machine))\n",
    "        state_list.append(state)\n",
    "        \n",
    "        #go to successor state\n",
    "        state = next_state\n",
    "        n_state = sum(state.jobs_remaining) #get job number of state\n",
    "        m_state = sum(state.machines_on_duty) #get machine number of state\n",
    "        \n",
    "    #if desired, compute optimal policy for end of schedule\n",
    "    if opt_end:\n",
    "        remaining_policy = compute_remaining_policy(state)\n",
    "        action_list += remaining_policy[0]\n",
    "        state_list += remaining_policy[1]\n",
    "    #elsewise, append final state\n",
    "    else:\n",
    "        state_list.append(state)\n",
    "        \n",
    "    return action_list, state_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef0db8",
   "metadata": {},
   "source": [
    "Having a policy, we can determine the costs its induced schedule produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_costs(policy):\n",
    "    pol_costs = sum(state.costs for state in policy[1])\n",
    "    return pol_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbdf71f",
   "metadata": {},
   "source": [
    "### Estimate New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d34df",
   "metadata": {},
   "source": [
    "Since we can now create a schedule with our Neural Network, we can use this to estimate net data with higher numbers of Jobs.<br>\n",
    "We already have trained our Neural Network on Job Scheduling Problems with 8 Jobs and 4 Machines. We will call this Network the Supervised Target Network. The idea is therefore to estimate data of problems with 9 Jobs and 4 Machines. To do this, we will randomly create such problem environments. Then, we will compute the transition cost for every action of assigning of the 9 Jobs in the beginning and the successor state. Since this one olny has 8 pending Jobs, we can estimate the remaining optimal schedule costs from there with our Neural Network. We create the schedule it thinks to be optimal and calculate the corresponding costs.<br>\n",
    "For the action of turning of a Machine, we compute the transition costs and the belonging successor state as well. From there we iteratively continue until reaching the state of 9 Jobs and 1 working Machine. From here, deactivating the currently free Machine is no longer an option. Therefore, the set of feasible actions is given by the 9 Job assignments. Doing as stated above we can then compute the estimated scheduling costs for every of these actions. Recursively we can then compute all these costs for the state of 9 Jobs and 2 Machines, 9 Jobs and 3 Machines and finally 9 Jobs and 4 Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc74f3",
   "metadata": {},
   "source": [
    "We can then use this data to train our Network, obtaining a new Target Network. We can then iteratively estimate data of increasing numbers of Jobs. To estimate the optimal actions whenever more than 8 Jobs are left, we use this uptrained Target Network. As soon as the number of Jobs get down to 8 Jobs or less, we use the Supervised Target Network. Whenever we receive estimated data for a higher number of Jobs than before, we uptrain the Target Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3d237",
   "metadata": {},
   "source": [
    "We will first give a function to estimate the costs of every Job assignment in the way described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate action costs for every job assignment with Neural Network\n",
    "def estim_assignment_costs(Target_NN, Sup_Tar_NN, state):\n",
    "    for action, job in enumerate(list_jobs):\n",
    "        #currently free Machine as object\n",
    "        machine = list_machines[state.machine]\n",
    "        #create successor state for every job assignment\n",
    "        successor_state = assign_job(state,job,machine)\n",
    "        #get transition costs\n",
    "        trans_costs = successor_state.costs\n",
    "        #estimate optimate policy from there with uptrained and supervised target NN\n",
    "        policy = create_policy(Target_NN, Sup_Tar_NN, successor_state)\n",
    "        #get the associated estimated optimal future costs\n",
    "        future_costs = policy_costs(policy)\n",
    "        #estimated action values are transition costs + estimated optimal future costs\n",
    "        state.Qvalues[action] = trans_costs + future_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201eb46",
   "metadata": {},
   "source": [
    "As stated, we have to compute all Machine-turn-off-successor-states and apply the previous function to estimate the action values for the Job assignments for each of these states, we can get the estimated Q-values for the actions of deactivating the currently free Machine as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b982a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate all Q-values\n",
    "def estim_Qvalues(Target_NN, Sup_Tar_NN, state):\n",
    "    \n",
    "    #estimate Q-values for all job assignments of initial state\n",
    "    estim_assignment_costs(Target_NN, Sup_Tar_NN, state)\n",
    "    #number of Machines of current state\n",
    "    m_state = sum(state.machines_on_duty)\n",
    "    #as long as more than 1 Machine is active, shutting it down is a feasible action\n",
    "    if m_state > 1:\n",
    "        #currently free Machine as object\n",
    "        machine = list_machines[state.machine]\n",
    "        #compute successor state of turning it off\n",
    "        turn_off_state = turn_off_machine(state, machine)\n",
    "        #estimate all Q-values\n",
    "        estim_Qvalues(Target_NN, Sup_Tar_NN, turn_off_state)\n",
    "        #transition costs to this successor state \n",
    "        trans_costs = turn_off_state.costs\n",
    "        #recursively define Q-value of shutting down free Machine\n",
    "        state.Qvalues[-1] = trans_costs + min(turn_off_state.Qvalues)\n",
    "    else:\n",
    "        #if only 1 Machine is active, only Job assignments are feasible actions\n",
    "        state.Qvalues = state.Qvalues[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46280b7",
   "metadata": {},
   "source": [
    "Being able to estimate the Q-values, we can now estimate new data. We want to create it for the states with 9 Jobs and 4,3 and 2 Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate data of increased number of Jobs\n",
    "def estim_data(data_dictionary, Target_NN, Sup_Tar_NN, init_state):\n",
    "    #initial state represents Job Scheduling Problem\n",
    "    state = init_state\n",
    "    #start with m Machines\n",
    "    m_state = m\n",
    "    #estimate Q_values for this state and all machine turn off successor states\n",
    "    estim_Qvalues(Target_NN, Sup_Tar_NN, state)\n",
    "    #list of states whose data we estimate\n",
    "    states = []\n",
    "    while m_state>1:\n",
    "        #compute input data of state\n",
    "        state_input(state)\n",
    "        #add to dictionary\n",
    "        data_dictionary[(n,m_state)][0].append(state.input)\n",
    "        #compute estimated target data of state \n",
    "        state_target(state)\n",
    "        #add to dictionary\n",
    "        data_dictionary[(n,m_state)][1].append(state.target[2])\n",
    "        #transition to successor state by turning off Machine\n",
    "        state = state.transition_dic[(n,state.machine)]\n",
    "        #upate number of working Machines\n",
    "        m_state -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a320e3",
   "metadata": {},
   "source": [
    "We want to to this to several simulated Job Scheduling Problems and create a dictionary of estimated data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4590e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of estimated data for simulated problems of increased job number\n",
    "def create_estim_data(Target_NN, Sup_Tar_NN, num_data):\n",
    "    #init dictionary for increased number of jobs and 2 up to m machines\n",
    "    data_dictionary = dict(((n,m_state),[[],[]]) \n",
    "                       for m_state in range(2,m+1))\n",
    "    #num_data stands for number of job scheduling problems we want to simulate to estimate the corresponding data\n",
    "    for _ in range(num_data):\n",
    "        #create random list of n jobs and m machines\n",
    "        create_JS_environment()\n",
    "        #initial state represents this job scheduling problem\n",
    "        init_state = create_initial_state()\n",
    "        #estimate its data\n",
    "        estim_data(data_dictionary, Target_NN, Sup_Tar_NN, init_state)\n",
    "        \n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcaa5d0",
   "metadata": {},
   "source": [
    "Now, we only need a function to save this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e857b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save estimated dictionary\n",
    "def save_estim_dictionary(data_dictionary, path, data_ind):\n",
    "    #save it under path and give the data an index\n",
    "    with open(f'{path}estim_data_{n}_Jobs_{data_ind}.pickle', 'wb') as f:\n",
    "        pickle.dump(data_dictionary, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b5057",
   "metadata": {},
   "source": [
    "### Compare Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77355de8",
   "metadata": {},
   "source": [
    "We need a measurement to evaluate how good the schedules produced by our Neural Network are.<br>\n",
    "For up to 8 Jobs, we can compare it to the optimal costs. However, for higher numbers of jobs this becomes to expensive. Therefore, we define a competetive heuristic scheduling algorithm. As for the Neural Network, we give the option to brute force compute the optimal schedule as soon as less than 3 Jobs remain or only 1 Machine is still working while not more than 8 Jobs remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#heuristic algorithm as comparision\n",
    "def comparative_metric(initial_state, opt_end=True, n_min=3, m_min=1):\n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    state = initial_state\n",
    "    \n",
    "    n_state = sum(state.jobs_remaining) #get job number of state\n",
    "    m_state = sum(state.machines_on_duty) #get machine number of state\n",
    "    \n",
    "    #minimum number of jobs and machines depends on whether optimal schedule shall be computed in the end or not\n",
    "    n_min = max(n_min*opt_end,1)\n",
    "    m_min = m_min*opt_end\n",
    "    \n",
    "    #loop through schedule until finished\n",
    "    while n_state >= n_min and (m_state >= m_min or n_state > 8):\n",
    "        \n",
    "        #create permutations\n",
    "        if not state.input:\n",
    "            seq_data(state)\n",
    "            \n",
    "        #list of remaining jobs\n",
    "        remaining_jobs = [job for job in list_jobs if state.jobs_remaining[job.index] == 1]\n",
    "        #currently free machine as object\n",
    "        machine = list_machines[state.machine]\n",
    "        #we will see if we assign a job in this situation\n",
    "        job_assigned = False\n",
    "        \n",
    "        #jobs get sorted first\n",
    "        for action in state.permutation[0]:\n",
    "            #iteratively try all jobs until fits\n",
    "            job = remaining_jobs[action]\n",
    "            #processing time on the currently free machine\n",
    "            proc_time = job.processing_time[state.machine]\n",
    "            #time until the job would be finished on any other machine is their occupation time + the processing time\n",
    "            alt_times = [state.machine_runtimes[i] + job.processing_time[i] for i in range(m) if state.machines_on_duty[i]==1]\n",
    "            #if job would not finish earlier on any other machine:\n",
    "            if not min(alt_times) < proc_time:\n",
    "                #assign job to currently free machine\n",
    "                next_state = assign_job(state, job, machine)\n",
    "                #a job got assigned\n",
    "                job_assigned = True\n",
    "                         \n",
    "        #if no job got assigned, machine has to be turned off\n",
    "        if job_assigned == False:\n",
    "            next_state = turn_off_machine(state, machine)\n",
    "            action = len(remaining_jobs)\n",
    "        \n",
    "        action_list.append((action,state.machine))\n",
    "        state_list.append(state)\n",
    "        \n",
    "        state = next_state\n",
    "        n_state = sum(state.jobs_remaining) #get job number of state\n",
    "        m_state = sum(state.machines_on_duty) #get machine number of state\n",
    "        \n",
    "    \n",
    "    #if desired, compute optimal policy for end of schedule\n",
    "    if opt_end:\n",
    "        remaining_policy = compute_remaining_policy(state)\n",
    "        action_list += remaining_policy[0]\n",
    "        state_list += remaining_policy[1]\n",
    "    #elsewise, append final state\n",
    "    else:\n",
    "        state_list.append(state)\n",
    "        \n",
    "    return action_list, state_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a5ab7",
   "metadata": {},
   "source": [
    "We now give a function to compare the scheduling costs of several scheduling approaches overa number of Job Scheduling Problems.<br>\n",
    "We can decide the following options:\n",
    "\n",
    "| option ||||| meaning |\n",
    "| :---: | --- | --- | --- | --- | :---: |\n",
    "| comp_opt_pol ||||| Do we want to compute the optimal policy? |\n",
    "| always_comp_opt_end ||||| Do we only want to use the version of the scheduling algorith with the optimal end computed or do we want both? |\n",
    "| use_uptr_tar_NN ||||| Do we want to usa an uptrained target Network (from estimated data of higher job numbers) as well? |\n",
    "\n",
    "The number of Job Scheduling Problems we want to simulate is then given by <i>num_schedules</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26557030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare scheduling costs. NN is the uptrained target NN\n",
    "def compare_schedules(NN, Sup_Tar_NN, num_schedules, comp_opt_pol=False, always_comp_opt_end=True, use_uptr_tar_NN=False):\n",
    "    \n",
    "    #most types of policies we can compute is 7\n",
    "    avg_cost_ratios = np.array([0]*7,dtype=np.float64)\n",
    "    \n",
    "    for _ in range(num_schedules):\n",
    "        #create random problem environment\n",
    "        create_JS_environment()\n",
    "        #create job scheduling problem from it\n",
    "        initial_state = create_initial_state()\n",
    "        #init list of scheduling costs\n",
    "        scheduling_costs = [0]*7\n",
    "        \n",
    "        #if optimal schedule shall be computed\n",
    "        if comp_opt_pol == True:\n",
    "            #create all states\n",
    "            all_states = create_all_states()\n",
    "            #compute all Q-values\n",
    "            backtracking(all_states)\n",
    "            #policy\n",
    "            opt_policy = optimal_policy(all_states[0])\n",
    "            #costs\n",
    "            scheduling_costs[0] = policy_costs(opt_policy)\n",
    "            \n",
    "        #policy and costs for Supervised NN policy with end being optimally computed\n",
    "        Sup_Tar_NN_policy_opt = create_policy(Sup_Tar_NN, Sup_Tar_NN, initial_state)\n",
    "        scheduling_costs[1] = policy_costs(Sup_Tar_NN_policy_opt)\n",
    "\n",
    "        #if we do not always want to compute the optimal end\n",
    "        if always_comp_opt_end == False:\n",
    "            #policy and costs for Supervised NN policy without end being optimally computed\n",
    "            Sup_Tar_NN_policy = create_policy(Sup_Tar_NN, Sup_Tar_NN, initial_state, opt_end=False)\n",
    "            scheduling_costs[2] = policy_costs(Sup_Tar_NN_policy)\n",
    "\n",
    "        #if we want to use an uptrained NN as well\n",
    "        if use_uptr_tar_NN == True:\n",
    "            \n",
    "            #policy and costs for Uptrained NN policy with end being optimally computed\n",
    "            Uptr_NN_policy_opt = create_policy(NN, Sup_Tar_NN, initial_state)\n",
    "            scheduling_costs[3] = policy_costs(Uptr_NN_policy_opt)\n",
    "\n",
    "            #if we do not always want to compute the optimal end\n",
    "            if always_comp_opt_end == False:\n",
    "                #policy and costs for Uptrained NN policy without end being optimally computed\n",
    "                Uptr_NN_policy = create_policy(NN, Sup_Tar_NN, initial_state, opt_end=False)\n",
    "                scheduling_costs[4] = policy_costs(Uptr_NN_policy)\n",
    "        \n",
    "        #policy and costs for comparative metric algorithm with end being optimally computed\n",
    "        comp_metric_policy_opt = comparative_metric(initial_state)\n",
    "        scheduling_costs[5] = policy_costs(comp_metric_policy_opt)\n",
    "        \n",
    "        #if we do not always want to compute the optimal end\n",
    "        if always_comp_opt_end == False:\n",
    "            #policy and costs for comparative metric algorithm without end being optimally computed\n",
    "            comp_metric_policy = comparative_metric(initial_state, opt_end=False)\n",
    "            scheduling_costs[6] = policy_costs(comp_metric_policy)\n",
    "        \n",
    "\n",
    "        scheduling_costs = np.array(scheduling_costs)\n",
    "        #scale either by optimal costs or by costs produced by supervised target NN with optimal end\n",
    "        if comp_opt_pol == True:\n",
    "            avg_cost_ratios += scheduling_costs / scheduling_costs[0]\n",
    "        else:\n",
    "            avg_cost_ratios += scheduling_costs / scheduling_costs[1]\n",
    "    \n",
    "    #scale by number of simulated job scheduling problems\n",
    "    avg_cost_ratios /= num_schedules\n",
    "    \n",
    "    #print results\n",
    "    print(f\"{n} Jobs and {m} Machines average costs ratio for {num_schedules} Problems:\")\n",
    "    policy_names = [\"Optimal Policy\", \"Supervised Network with optimal End\", \"Supervised Network\", \"Uptrained Network with optimal End\", \"Uptrained Network\", \"Heuristic Algo with optimal End\", \"Heuristic Algo\"]\n",
    "    for i, avg_cost_ratio in enumerate(avg_cost_ratios):\n",
    "        if avg_cost_ratio > 0:\n",
    "            print(round(avg_cost_ratio,2), \"-\", policy_names[i])\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
