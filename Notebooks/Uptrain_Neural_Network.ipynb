{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f57783",
   "metadata": {},
   "source": [
    "# Uptrained Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb77c0",
   "metadata": {},
   "source": [
    "## Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47cee93",
   "metadata": {},
   "source": [
    "In this Notebook we will uptrain our Neural Network on an increased number of Jobs.<br>\n",
    "If this is the first iteration, we will uptrain the Supervised Neural Network to 9 Jobs.<br>\n",
    "If he have uptrained it already before on more than 8 Jobs, we increase the maximum number of Jobs by one and continue training that uptrained version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7bb9aa",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321700a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Jobs_and_Machines.ipynb\n",
      "importing Jupyter notebook from States_and_Policies.ipynb\n",
      "importing Jupyter notebook from Global_Variables.ipynb\n",
      "importing Jupyter notebook from Action_Pointer.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, LSTM, Concatenate, LeakyReLU, Softmax, Dropout\n",
    "from keras.layers import Lambda, Flatten, Bidirectional, TimeDistributed, Reshape, MultiHeadAttention, LayerNormalization\n",
    "from keras.activations import tanh\n",
    "from keras.models import Model, Sequential\n",
    "import keras.backend\n",
    "import random\n",
    "from copy import copy\n",
    "#import necessary notebooks\n",
    "import import_ipynb\n",
    "from Jobs_and_Machines import *\n",
    "from States_and_Policies import *\n",
    "from Action_Pointer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493c9f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Job-Scheduling-Files'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change working directory\n",
    "os.chdir('D:\\\\Job-Scheduling-Files')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce12a0",
   "metadata": {},
   "source": [
    "### Create Estimated Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53335e0",
   "metadata": {},
   "source": [
    "Set the increased number of Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c201d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the new number of Jobs?\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "n_estim = int(input(\"What is the new number of Jobs?\\n\"))\n",
    "\n",
    "n = 8\n",
    "n_min = 3\n",
    "\n",
    "m = 4\n",
    "m_min = 2\n",
    "\n",
    "DS_max = 10\n",
    "DS_min = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acff0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(DS_min,DS_max):\n",
    "    \n",
    "    #dictionary will contain data from all respective data sets\n",
    "    data_dict = dict(((n_state,m_state),[[],[]]) \n",
    "                           for n_state in range(n_min,n+1) for m_state in range(m_min,m+1))\n",
    "    \n",
    "    #load data\n",
    "    for n_state in range(n_min, n+1):\n",
    "        for m_state in range(m_min, m+1):\n",
    "            #resource information of the jobs: processing time on every machine and its metadata\n",
    "            x_res = []\n",
    "            #urgency information of the jobs: earliness and weight\n",
    "            x_urg = []\n",
    "            #targets\n",
    "            y = []\n",
    "            #loop over every data set \"DS\"\n",
    "            for DS in [\"0\"*(2-len(str(i)))+str(i) for i in range(DS_min,DS_max+1)]:\n",
    "                training_data_path = f'Data/DataSet_{DS}/LSTM_Data_RR_{DS}/{n_state}-jobs-{m_state}-machines_{DS}.pickle'\n",
    "                with open(training_data_path, 'rb') as f:\n",
    "                    #load data set of key (n_state,m_state)\n",
    "                    data = pickle.load(f)\n",
    "                    #get data about its resources for every job of one state of one Job Scheduling Problem\n",
    "                    jobs_resources = data[0][0][:,:,:-2].reshape((-1,n_state,m_state,4))\n",
    "                    #get data abouts its urgenicies for every job  of one state of one Job Scheduling Problem\n",
    "                    jobs_urgencies = data[0][0][:,:,-2:]\n",
    "                    #add to data of states of other Job Scheduling Problems\n",
    "                    x_res.append(jobs_resources)\n",
    "                    x_urg.append(jobs_urgencies)\n",
    "                    y.append(data[1][0])\n",
    "            #transform list of data from states of Job Scheduling Problems to numpy array and add to final data dictionary\n",
    "            data_dict[(n_state,m_state)][0].append(np.concatenate(x_res))\n",
    "            data_dict[(n_state,m_state)][0].append(np.concatenate(x_urg))\n",
    "            data_dict[(n_state,m_state)][1].append(np.concatenate(y))\n",
    "            \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27669216",
   "metadata": {},
   "source": [
    "We will now load the estimated data and transform it into the form of training data.<br>\n",
    "Note that the path in the function might have to be changed by user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "114dc727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load estimated data\n",
    "def load_estim_data():\n",
    "    \n",
    "    #dictionary will contain all estimated\n",
    "    estim_data_dict = dict(((n_state,m_state),[[],[]]) \n",
    "                           for n_state in range(n+1, n_estim+1) for m_state in range(m_min,m+1))\n",
    "    \n",
    "    #load estimated data from n+1 to n_estim\n",
    "    for n_state in range(n, n_estim+1):\n",
    "        for m_state in range(m_min, m+1):\n",
    "            #resource information of the jobs: processing time on every machine and its metadata\n",
    "            x_res = []\n",
    "            #urgency information of the jobs: earliness and weight\n",
    "            x_urg = []\n",
    "            #targets\n",
    "            y = []\n",
    "            \"\"\"use your own path here\"\"\"\n",
    "            estim_data_path = f'Data/EstimData/{n_state}_Jobs/LSTM_EstimData_RR/{n_state}-jobs-{m_state}-machines.pickle'\n",
    "            with open(estim_data_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                #get data about its resources for every job of one state of one Job Scheduling Problem\n",
    "                jobs_resources = data[0][0][:,:,:-2].reshape((-1,n_state,m_state,4))\n",
    "                #get data abouts its urgenicies for every job  of one state of one Job Scheduling Problem\n",
    "                jobs_urgencies = data[0][0][:,:,-2:]\n",
    "                #add to data of states of other Job Scheduling Problems\n",
    "                x_res.append(jobs_resources)\n",
    "                x_urg.append(jobs_urgencies)\n",
    "                y.append(data[1][0])\n",
    "\n",
    "            #transform list of data from states of Job Scheduling Problems to numpy array and add to final data dictionary\n",
    "            estim_data_dict[(n_state,m_state)][0].append(np.concatenate(x_res))\n",
    "            estim_data_dict[(n_state,m_state)][0].append(np.concatenate(x_urg))\n",
    "            estim_data_dict[(n_state,m_state)][1].append(np.concatenate(y))\n",
    "\n",
    "    return estim_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f2086",
   "metadata": {},
   "source": [
    "Load the estimated data and bring it into the correct form by splitting it every <i>(n_state,m_state)</i>-subset from it into a list of inputs and a list of targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ee2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estim_dict = load_estim_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbd36186",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_estim_list, y_estim_list = [], []\n",
    "for key in estim_dict:\n",
    "    \n",
    "    #estimated data\n",
    "    x_estim, y_estim = estim_dict[key]\n",
    "    x_estim_list.append(x_estim)\n",
    "    y_estim_list.append(y_estim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a2c67",
   "metadata": {},
   "source": [
    "Load Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6abb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load desired version of Neural Network\n",
    "def load_NN(NN_name):\n",
    "    \"\"\"path might have to be updated based on users storage\"\"\"\n",
    "    path = 'D:\\\\Job-Scheduling-Files\\'\n",
    "    \n",
    "    NN = keras.models.load_model(f'{path}{NN_name}.h5', custom_objects={'FeedForward': FeedForward, 'Pointer': Pointer, 'MSE_with_Softmax': MSE_with_Softmax, 'costs':costs})\n",
    "    NN.run_eagerly = True\n",
    "    \n",
    "    return NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d18f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uptrained_Neural_Network = load_NN(f\"Final_Pointer3\")\n",
    "Uptrained_Neural_Network = load_NN(f\"Uptrained_Final_Pointer_{n_estim-1}_Jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c9d39",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ec2f3",
   "metadata": {},
   "source": [
    "Compile the model the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d3a6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "Neural_Network.compile(\n",
    "        #custom loss\n",
    "        loss = MSE_with_Softmax,\n",
    "        #optimizer\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "        run_eagerly=True,\n",
    "        #custom metric\n",
    "        metrics = [costs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775a49d",
   "metadata": {},
   "source": [
    "### Uptraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7a5d4",
   "metadata": {},
   "source": [
    "We will now use the estimated data to uptrain the Neural Network.<br>\n",
    "Again, one epoch will consist of the states related to one <i>(n_state,m_state)</i>-subset only.<br>\n",
    "Since is has been trained for any number of Jobs greater than <i>n</i> but lower than <i>n_estim</i> already, we will first train it on the data of <i>n_estim</i> jobs. We loop 5 times through the data, resulting in 5*3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcf15af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train for 12 Jobs\n",
      "782/782 [==============================] - 1847s 2s/step - loss: 0.2005 - costs: 0.2665\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 554s 708ms/step - loss: 0.4380 - costs: 0.6844\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 564s 721ms/step - loss: 0.8190 - costs: 1.2095\n"
     ]
    }
   ],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=0*3\n",
    "#load the last three subsets, as these are (n_estim,4), (n_estim,3) and (n_estim,2)\n",
    "for j in range(len(x_estim_list)-3,len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3ba88d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train for 12 Jobs\n",
      "Epoch 4/4\n",
      "782/782 [==============================] - 532s 680ms/step - loss: 0.1972 - costs: 0.2604\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 547s 700ms/step - loss: 0.4212 - costs: 0.6698\n",
      "Epoch 6/6\n",
      "782/782 [==============================] - 564s 721ms/step - loss: 0.8041 - costs: 1.1892\n"
     ]
    }
   ],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=1*3\n",
    "#load the last three subsets, as these are (n_estim,4), (n_estim,3) and (n_estim,2)\n",
    "for j in range(len(x_estim_list)-3,len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eb99993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train for 12 Jobs\n",
      "Epoch 7/7\n",
      "782/782 [==============================] - 534s 683ms/step - loss: 0.1932 - costs: 0.2596\n",
      "Epoch 8/8\n",
      "782/782 [==============================] - 549s 702ms/step - loss: 0.4184 - costs: 0.6650\n",
      "Epoch 9/9\n",
      "782/782 [==============================] - 564s 722ms/step - loss: 0.7956 - costs: 1.1858\n"
     ]
    }
   ],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=2*3\n",
    "#load the last three subsets, as these are (n_estim,4), (n_estim,3) and (n_estim,2)\n",
    "for j in range(len(x_estim_list)-3,len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "743d9365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train for 12 Jobs\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 843s 1s/step - loss: 0.1930 - costs: 0.2555\n",
      "Epoch 11/11\n",
      "782/782 [==============================] - 554s 709ms/step - loss: 0.4143 - costs: 0.6491\n",
      "Epoch 12/12\n",
      "782/782 [==============================] - 566s 724ms/step - loss: 0.7870 - costs: 1.1666\n"
     ]
    }
   ],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=3*3\n",
    "#load the last three subsets, as these are (n_estim,4), (n_estim,3) and (n_estim,2)\n",
    "for j in range(len(x_estim_list)-3,len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45f24aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train for 12 Jobs\n",
      "Epoch 13/13\n",
      "782/782 [==============================] - 536s 685ms/step - loss: 0.1914 - costs: 0.2551\n",
      "Epoch 14/14\n",
      "782/782 [==============================] - 549s 702ms/step - loss: 0.4114 - costs: 0.6469\n",
      "Epoch 15/15\n",
      "782/782 [==============================] - 577s 738ms/step - loss: 0.7891 - costs: 1.1619\n"
     ]
    }
   ],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=4*3\n",
    "#load the last three subsets, as these are (n_estim,4), (n_estim,3) and (n_estim,2)\n",
    "for j in range(len(x_estim_list)-3,len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbfcaa5",
   "metadata": {},
   "source": [
    "If <i>n_estim > 9</i>, we retrain the Network on the entire estimated data set. One loop therefore consists of 3*i epochs.<br>\n",
    "We do <i>n_estim-n+1</i> loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "178d497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train for 12 Jobs\n",
      "Epoch 16/16\n",
      "782/782 [==============================] - 475s 607ms/step - loss: 0.3298 - costs: 0.2496\n",
      "Epoch 17/17\n",
      "782/782 [==============================] - 493s 631ms/step - loss: 0.8400 - costs: 0.5915\n",
      "Epoch 18/18\n",
      "782/782 [==============================] - 506s 647ms/step - loss: 1.4663 - costs: 0.9090\n",
      "Epoch 19/19\n",
      "782/782 [==============================] - 10812s 14s/step - loss: 0.2588 - costs: 0.2532\n",
      "Epoch 20/20\n",
      "782/782 [==============================] - 1630s 2s/step - loss: 0.6463 - costs: 0.6338\n",
      "Epoch 21/21\n",
      "782/782 [==============================] - 529s 676ms/step - loss: 1.1543 - costs: 1.0206\n",
      "Epoch 22/22\n",
      "782/782 [==============================] - 516s 659ms/step - loss: 0.2344 - costs: 0.2727\n",
      "Epoch 23/23\n",
      "782/782 [==============================] - 532s 680ms/step - loss: 0.5311 - costs: 0.6871\n",
      "Epoch 24/24\n",
      "782/782 [==============================] - 549s 702ms/step - loss: 0.9620 - costs: 1.1236\n",
      "Epoch 25/25\n",
      "782/782 [==============================] - 544s 695ms/step - loss: 0.1953 - costs: 0.2566\n",
      "Epoch 26/26\n",
      "782/782 [==============================] - 557s 712ms/step - loss: 0.4105 - costs: 0.6536\n",
      "Epoch 27/27\n",
      "782/782 [==============================] - 1317s 2s/step - loss: 0.7802 - costs: 1.1653\n"
     ]
    }
   ],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=5*3\n",
    "for j in range(len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22ba4061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train for 12 Jobs\n",
      "Epoch 28/28\n",
      "782/782 [==============================] - 481s 616ms/step - loss: 0.3215 - costs: 0.2416\n",
      "Epoch 29/29\n",
      "782/782 [==============================] - 487s 623ms/step - loss: 0.8265 - costs: 0.5744\n",
      "Epoch 30/30\n",
      "782/782 [==============================] - 516s 660ms/step - loss: 1.4457 - costs: 0.9109\n",
      "Epoch 31/31\n",
      "782/782 [==============================] - 500s 639ms/step - loss: 0.2534 - costs: 0.2544\n",
      "Epoch 32/32\n",
      "782/782 [==============================] - 1133s 1s/step - loss: 0.6399 - costs: 0.6263\n",
      "Epoch 33/33\n",
      "782/782 [==============================] - 510s 652ms/step - loss: 1.1443 - costs: 1.0105\n",
      "Epoch 34/34\n",
      "782/782 [==============================] - 515s 659ms/step - loss: 0.2343 - costs: 0.2693\n",
      "Epoch 35/35\n",
      "782/782 [==============================] - 528s 675ms/step - loss: 0.5283 - costs: 0.6822\n",
      "Epoch 36/36\n",
      "782/782 [==============================] - 554s 708ms/step - loss: 0.9493 - costs: 1.1060\n",
      "Epoch 37/37\n",
      "782/782 [==============================] - 539s 689ms/step - loss: 0.1926 - costs: 0.2580\n",
      "Epoch 38/38\n",
      "782/782 [==============================] - 461s 590ms/step - loss: 0.4135 - costs: 0.6605\n",
      "Epoch 39/39\n",
      "782/782 [==============================] - 461s 590ms/step - loss: 0.7756 - costs: 1.1556\n"
     ]
    }
   ],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=5*3 + len(x_estim_list)\n",
    "for j in range(len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85ac2ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train for 12 Jobs\n",
      "Epoch 40/40\n",
      "782/782 [==============================] - 277s 354ms/step - loss: 0.3210 - costs: 0.2446\n",
      "Epoch 41/41\n",
      "782/782 [==============================] - 320s 409ms/step - loss: 0.8180 - costs: 0.5744\n",
      "Epoch 42/42\n",
      "782/782 [==============================] - 328s 420ms/step - loss: 1.4365 - costs: 0.8985\n",
      "Epoch 43/43\n",
      "782/782 [==============================] - 343s 438ms/step - loss: 0.2556 - costs: 0.2492\n",
      "Epoch 44/44\n",
      "782/782 [==============================] - 364s 465ms/step - loss: 0.6369 - costs: 0.6227\n",
      "Epoch 45/45\n",
      "782/782 [==============================] - 332s 424ms/step - loss: 1.1354 - costs: 1.0137\n",
      "Epoch 46/46\n",
      "782/782 [==============================] - 299s 382ms/step - loss: 0.2330 - costs: 0.2727\n",
      "Epoch 47/47\n",
      "782/782 [==============================] - 308s 394ms/step - loss: 0.5285 - costs: 0.6851\n",
      "Epoch 48/48\n",
      "782/782 [==============================] - 318s 407ms/step - loss: 0.9444 - costs: 1.1141\n",
      "Epoch 49/49\n",
      "782/782 [==============================] - 312s 399ms/step - loss: 0.1927 - costs: 0.2583\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 325s 415ms/step - loss: 0.4113 - costs: 0.6558\n",
      "Epoch 51/51\n",
      "782/782 [==============================] - 334s 427ms/step - loss: 0.7746 - costs: 1.1698\n"
     ]
    }
   ],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=5*3 + 2*len(x_estim_list)\n",
    "for j in range(len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9709ce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train for 12 Jobs\n",
      "Epoch 52/52\n",
      "782/782 [==============================] - 273s 348ms/step - loss: 0.3169 - costs: 0.2413\n",
      "Epoch 53/53\n",
      "782/782 [==============================] - 285s 364ms/step - loss: 0.8231 - costs: 0.5726\n",
      "Epoch 54/54\n",
      "782/782 [==============================] - 292s 374ms/step - loss: 1.4126 - costs: 0.8937\n",
      "Epoch 55/55\n",
      "782/782 [==============================] - 287s 367ms/step - loss: 0.2553 - costs: 0.2479\n",
      "Epoch 56/56\n",
      "782/782 [==============================] - 296s 379ms/step - loss: 0.6341 - costs: 0.6293\n",
      "Epoch 57/57\n",
      "782/782 [==============================] - 306s 391ms/step - loss: 1.1296 - costs: 1.0089\n",
      "Epoch 58/58\n",
      "782/782 [==============================] - 299s 383ms/step - loss: 0.2309 - costs: 0.2719\n",
      "Epoch 59/59\n",
      "782/782 [==============================] - 309s 396ms/step - loss: 0.5265 - costs: 0.6864\n",
      "Epoch 60/60\n",
      "782/782 [==============================] - 319s 407ms/step - loss: 0.9427 - costs: 1.0996\n",
      "Epoch 61/61\n",
      "782/782 [==============================] - 315s 402ms/step - loss: 0.1918 - costs: 0.2588\n",
      "Epoch 62/62\n",
      "782/782 [==============================] - 324s 414ms/step - loss: 0.4100 - costs: 0.6529\n",
      "Epoch 63/63\n",
      "782/782 [==============================] - 336s 429ms/step - loss: 0.7739 - costs: 1.1568\n"
     ]
    }
   ],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=5*3 + 3*len(x_estim_list)\n",
    "for j in range(len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9feb644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train for 12 Jobs\n",
      "Epoch 64/64\n",
      "782/782 [==============================] - 458s 585ms/step - loss: 0.3117 - costs: 0.2405\n",
      "Epoch 65/65\n",
      "782/782 [==============================] - 507s 648ms/step - loss: 0.8429 - costs: 0.5788\n",
      "Epoch 66/66\n",
      "782/782 [==============================] - 6947s 9s/step - loss: 1.4117 - costs: 0.8779\n",
      "Epoch 67/67\n",
      "782/782 [==============================] - 3342s 4s/step - loss: 0.2493 - costs: 0.2465\n",
      "Epoch 68/68\n",
      "782/782 [==============================] - 523s 668ms/step - loss: 0.6341 - costs: 0.6203\n",
      "Epoch 69/69\n",
      "782/782 [==============================] - 549s 702ms/step - loss: 1.1254 - costs: 0.9920\n",
      "Epoch 70/70\n",
      "782/782 [==============================] - 547s 699ms/step - loss: 0.2310 - costs: 0.2662\n",
      "Epoch 71/71\n",
      "782/782 [==============================] - 566s 724ms/step - loss: 0.5242 - costs: 0.6855\n",
      "Epoch 72/72\n",
      "782/782 [==============================] - 580s 741ms/step - loss: 0.9385 - costs: 1.0960\n",
      "Epoch 73/73\n",
      "782/782 [==============================] - 579s 741ms/step - loss: 0.1926 - costs: 0.2547\n",
      "Epoch 74/74\n",
      "782/782 [==============================] - 9397s 12s/step - loss: 0.4077 - costs: 0.6460\n",
      "Epoch 75/75\n",
      "782/782 [==============================] - 342s 437ms/step - loss: 0.7697 - costs: 1.1574\n"
     ]
    }
   ],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=5*3 + 4*len(x_estim_list)\n",
    "for j in range(len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff087b",
   "metadata": {},
   "source": [
    "Finally, we save the uptrained version of the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd547450",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uptrained_Neural_Network.save(f'Uptrained_Neural_Network_{n_estim}_Jobs.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
