{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb307f17",
   "metadata": {},
   "source": [
    "# Action-Pointer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b8991",
   "metadata": {},
   "source": [
    "## Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48adb8b8",
   "metadata": {},
   "source": [
    "In this Notebook, we create the necessary layers for the Action-Pointer Decoder of our Neural Network as well as the loss function and metric that we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767325b2",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e394b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, LSTM, Concatenate, LeakyReLU, Softmax, Dropout\n",
    "from keras.layers import Lambda, Flatten, Bidirectional, TimeDistributed, Reshape, MultiHeadAttention, LayerNormalization\n",
    "from keras.activations import tanh\n",
    "from keras.models import Model, Sequential\n",
    "import keras.backend\n",
    "import random\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7bdee",
   "metadata": {},
   "source": [
    "### Feed Forward Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f27e63",
   "metadata": {},
   "source": [
    "First, we create a Feed Forward Layer that will be used throughout our Neural Network.<br>\n",
    "It consists consists of:\n",
    "- A first, inner Dense layer with dimension <i>inner_dim</i> and activation function Leaky ReLU with <i>alpha</i> = 0.1\n",
    "- A second, outer Dense layer with dimension <i>outer_dim</i> and no activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, inner_dim, outer_dim, alpha=0.1, name=None,**kwargs):\n",
    "        super(FeedForward, self).__init__(name=name,**kwargs)\n",
    "        #dimension of inner dense layer\n",
    "        self.inner_dim = inner_dim\n",
    "        #dimension of outer dense layer\n",
    "        self.outer_dim = outer_dim\n",
    "        #alpha activation for Leaky ReLU of inner dense layer\n",
    "        self.alpha = alpha\n",
    "        #inner dense layer\n",
    "        self.dense_in = Dense(self.inner_dim)\n",
    "        #outer dense layer\n",
    "        self.dense_out = Dense(self.outer_dim)\n",
    "        \n",
    "    def call(self, x):\n",
    "        #apply inner Dense laayer\n",
    "        x = self.dense_in(x)\n",
    "        #apply Leaky ReLU as activation function \n",
    "        x = LeakyReLU(self.alpha)(x)\n",
    "        #apply outer Dense layer\n",
    "        x = self.dense_out(x)\n",
    "        return x\n",
    "   \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"outer_dim\": self.outer_dim,\n",
    "            \"inner_dim\": self.inner_dim,\n",
    "            \"alpha\": self.alpha\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a755e",
   "metadata": {},
   "source": [
    "### Pointer Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f28082",
   "metadata": {},
   "source": [
    "Now, we create the attention mechanism of our Pointer Network. It is based on the Paper \"Pointer Networks\" in 2015.<br>\n",
    "A sequence will be given as an input alongside a vector that has been created from that sequence by an LSTM.<br>\n",
    "Apply a learnable matrix weight matrix <i>W2</i> to this vector.<br>\n",
    "Then, for every vector of the sequence, apply a learnable weight matrix <i>W1</i>.<br>\n",
    "Afterwards, sum the results and apply the function <i>tanh</i>.<br>\n",
    "Finally, take its dot product with a learnable weight vector <i>v</i>.<br>\n",
    "The output is then a vector of the attentions over all elements of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pointer(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dim, name=None,**kwargs):\n",
    "        super(Pointer, self).__init__(name=name,**kwargs)\n",
    "        self.dim = dim\n",
    "        #apply learnable weight matrix W1\n",
    "        self.W1 = Dense(dim, use_bias=False)\n",
    "        #apply learnable weight matrix W2\n",
    "        self.W2 = Dense(dim, use_bias=False)\n",
    "        #learnable weight vector \"v\"\n",
    "        self.V = Dense(1, use_bias=False)\n",
    "        \n",
    "    def call(self, enc_outputs, dec_output):\n",
    "        #apply W1 to element of input sequence\n",
    "        w1_e = self.W1(enc_outputs)\n",
    "        #apply W2 to vector generated from LSTM by entire sequence\n",
    "        w2_d = Reshape((1,-1))(self.W2(dec_output))\n",
    "        #sum and apply tanh\n",
    "        tanh_out = tanh(w1_e + w2_d)\n",
    "        #take dot-product with vector \"v\"\n",
    "        attention = self.V(tanh_out)\n",
    "        #output is vector of attentions over all elements of input sequence\n",
    "        out = Flatten()(attention)\n",
    "        return out\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"dim\": self.dim\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff77ca",
   "metadata": {},
   "source": [
    "### Metric and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383427d",
   "metadata": {},
   "source": [
    "To compile our model, we will define a custom metric and a custom loss function.<br>\n",
    "The custom metric shall measure how much a decision of our Neural Network costs.<br>\n",
    "In specific, it denotes how much additional relative costs arise by taking the action estimated to be optimal by our Neural Network in a state instead of taking the true optimal action. For this, we assume that the optimal policy would be applied from each successor state on, so the chosen action of our Neural Network for the current state would be the only disruption of optimality with regards to the schedule.<br>\n",
    "The metric is therefore defined as the quotient of the Q-value of the indicated and the optimal action.<br>\n",
    "We do also substract 1 and multiply the factor 100, so that the final number stands for the relative addition in costs as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4853c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true are the real normalized Q-values, y_pred the predicted ones\n",
    "#since the Q-values got normalized, the action with the highest target value corresponds to the lowest Q-value \n",
    "def costs(y_true, y_pred):\n",
    "    \n",
    "    #take the indices of the actions estimated to be optimal by our Neural Network\n",
    "    indices = keras.backend.argmax(y_pred, axis=1)\n",
    "    length = np.shape(indices)[0]\n",
    "    #get the true normalized target values associated with these actions\n",
    "    inv_values = keras.backend.eval(y_true)[np.arange(length),indices]\n",
    "    #the normalized target values are exactly the quotient of the optimal and the chosen Q-values\n",
    "    #therefore, their inverse denotes exactly the relative additional costs\n",
    "    Q_factors = 1/inv_values\n",
    "    #average over all considered states\n",
    "    Q_factor = np.mean(Q_factors)\n",
    "    #give only relative cost increase as percentage\n",
    "    Q_factor = (Q_factor - 1)*100\n",
    "    \n",
    "    return Q_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863634b",
   "metadata": {},
   "source": [
    "For the loss function we will use the mean-squarred-error.<br>\n",
    "Note, however, that we have not applied a softmax when creating the data nor the Neural Network. Instead, we will do this within the loss function itself, so we have to safe the data in less formats, since we need the target values and predictions before the softmax for our custom metric above. Consequently, we need to define a custom loss function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55843a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_with_Softmax(y_pred, y_true):\n",
    "    #apply softmax\n",
    "    y_pred = Softmax()(y_pred)\n",
    "    y_true = Softmax()(y_true)\n",
    "    #loss is MSE\n",
    "    loss = keras.losses.MeanSquaredError()(y_pred,y_true)\n",
    "    #loss will be small, so upscale for better readability\n",
    "    return (loss * 10e4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
