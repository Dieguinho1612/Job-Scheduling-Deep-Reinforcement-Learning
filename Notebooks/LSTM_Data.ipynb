{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ec7610",
   "metadata": {},
   "source": [
    "# Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b086d",
   "metadata": {},
   "source": [
    "This Notebook applies some technical transformations to given data.<br>\n",
    "The goal is to merge the data of a directory and bring it into a form that can directly be fed to our Neural Network.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2fd08",
   "metadata": {},
   "source": [
    "For every key, corresponding to the 2-tuple of the amounts of remaining Jobs and Machines, we will:\n",
    "   1. load lists of data samples assigned to this key throughout all dictionaries within a directory\n",
    "   2. for every such list, select some of these samples with regards to a balancing rule \n",
    "   3. transform the numpy-arrays into LSTM compatible form\n",
    "   4. concatenate them to a final list\n",
    "    \n",
    "Finally, we will store each of these final lists as a pickle-list in a sub-directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb3045",
   "metadata": {},
   "source": [
    "So after creating a directory of data dictionaries, this Notebook has to be run on it.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb2580",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d3ba01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Jobs_and_Machines.ipynb\n",
      "importing Jupyter notebook from States_and_Policies.ipynb\n",
      "importing Jupyter notebook from Global_Variables.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import import_ipynb\n",
    "from Jobs_and_Machines import *\n",
    "from States_and_Policies import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb5b43",
   "metadata": {},
   "source": [
    "### Select Hyperparameters and Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a3117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set key limits\n",
    "n = 8 #max job number\n",
    "m = 4 #max machine number\n",
    "n_min = 3 #min job number\n",
    "m_min = 2 #min machine number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa5134b",
   "metadata": {},
   "source": [
    "A directory corresponds to a data set. So choose the data set that shall be transformed.  To use estimated data of higher Job numbers, resulting from the approach of applying further techniques of Deep Reinforcement Learning, respond <i>0</i> in the following input-question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e22a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Data-Set do you want to work on?\n",
      "Type 0 if you want to work on estimated Data-Sets.\n",
      "01\n"
     ]
    }
   ],
   "source": [
    "#Number of Data Set has to be given in two digits, so for example \"1\" has to beg given as \"01\", while \"98\" stays the same\n",
    "DS = input(\"Which Data-Set do you want to work on?\\n\"+\"Type 0 if you want to work on estimated Data-Sets.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca98126f",
   "metadata": {},
   "source": [
    "In case that the data set is estimated for a higher number of Jobs, this increased number has to be stated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ead42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DS == \"0\":\n",
    "    n = int(input(\"What is the new number of Jobs?\\n\"))\n",
    "    n_min = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f6d78",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf8763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to merge dictionaries containing data\n",
    "def merge_dicts(data_dict, sample_dict):\n",
    "    for key in data_dict:\n",
    "        if key in sample_dict:\n",
    "            data_dict[key][0] += sample_dict[key][0] #add inputs\n",
    "            data_dict[key][1] += sample_dict[key][1] #add targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eebbd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring data into format directly readable for Neural Network and its base layer, being an LSTM\n",
    "def data_into_LSTM_format(data_dict):\n",
    "    #iterate over keys\n",
    "    for key in data_dict:\n",
    "        #get inputs and targets\n",
    "        inputs, targets = data_dict[key] #list of list of inputs and list of targets\n",
    "        #inputs is a list, for every state their is one entry, being a list itself \n",
    "        #These inner lists consist of two entries: Job-data and Machine-data of a state\n",
    "        #every machine-data consists of 3 entries, so create indexes for the range of m_state repeating every index 3 times\n",
    "        idxs = [ind+1 for ind in range(key[1]) for _ in range(3)]\n",
    "        \n",
    "        #inputs are now Jobs. Each Job is a sequence of the processing time and the respective machine information.\n",
    "        #the last 2 entries are the jobs earliness and weight\n",
    "        seq_inputs = [np.insert(inp[0],idxs,inp[1].flatten(), axis=1) for inp in inputs]\n",
    "        \n",
    "        #merge samples to numpy array\n",
    "        data_dict[key][0] = [np.stack(seq_inputs)]\n",
    "        data_dict[key][1] = [np.stack(targets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635fdd8a",
   "metadata": {},
   "source": [
    "### Create Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e53a37e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the working path to the data directory?\n",
      "D:\\\\Job-Scheduling-Files\\Data\n"
     ]
    }
   ],
   "source": [
    "#change working path to Data directory\n",
    "work_path = input(\"What is the working path to the data directory?\\n\")\n",
    "os.chdir(work_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b691468",
   "metadata": {},
   "source": [
    "We will now merge all data dictionaries of a directory by their keys into one final dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e629b1fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328 seconds to merge sample data into final dictionary\n"
     ]
    }
   ],
   "source": [
    "#merged dictionary\n",
    "data_dict = dict(((n_state,m_state),[[],[]]) \n",
    "                           for n_state in range(n_min,n+1) for m_state in range(m_min,m+1))\n",
    "\n",
    "#measure starting time\n",
    "st = time.time()\n",
    "\n",
    "#check if we are using estimated data set\n",
    "if DS == \"0\":\n",
    "    data_path = f'EstimData/{n}_Jobs/estim_data_{n}_Jobs'\n",
    "    data_indices = [str(i+1) for i in range(800)]\n",
    "#else get path to folder of data sets\n",
    "else:\n",
    "    #Directory of data. We used 10 of them for the training data\n",
    "    data_path = f'DataSet_{DS}/data_{DS}'\n",
    "    #every file consists of the dictionary of one Job Scheduling Problem\n",
    "    data_indices = [\"0\"*(4-len(str(i))) + str(i) for i in range((int(DS)-1)*10000,int(DS)*10000)] #10000\n",
    "\n",
    "#loop over dictionaries\n",
    "for data_ind in data_indices:\n",
    "    #open dictionary\n",
    "    with open(f'{data_path}_{data_ind}.pickle', 'rb') as f:\n",
    "        #load sample dictionary\n",
    "        sample_dict = pickle.load(f)\n",
    "        #loop over it keys\n",
    "        for key in sample_dict:\n",
    "            #down sample will contain selection of data\n",
    "            down_sample = [[],[]]\n",
    "            n_state = key[0]\n",
    "            #98 denotes the validation data, 99 the test data\n",
    "            if DS in [\"98\", \"99\"]:\n",
    "                #only one state-data per n-m-combination for every Job Scheduling Problem\n",
    "                data_length = 1\n",
    "                #take first data instance\n",
    "                down_sample[0] = sample_dict[key][0][:data_length]\n",
    "                down_sample[1] = sample_dict[key][1][:data_length]\n",
    "                #update current sample dictionary\n",
    "                sample_dict[key] = down_sample\n",
    "            #if training data dictionary\n",
    "            elif int(DS)-1 in range(10):\n",
    "                #target index \"i\" corresponds to \"i\" being the optimal action in a selected state\n",
    "                #we only select the data of on state for every such \"i\" from every dictionary=JobSchedulingProblem\n",
    "                target_indices = [0 for i in range(n_state+1)]\n",
    "                #loop over target-vectors\n",
    "                for i, row in enumerate(sample_dict[key][1]):\n",
    "                    #check if we already saved the data of a state whose optimal action is equal to the one of this target vector\n",
    "                    if target_indices[n_state - np.argmax(row[::-1])] == 0:\n",
    "                        #add input data\n",
    "                        down_sample[0].append(sample_dict[key][0][i])\n",
    "                        #add target values\n",
    "                        down_sample[1].append(row)\n",
    "                        #update that we already have one state with optimal action \"i\" for this dictionary\n",
    "                        target_indices[n_state - np.argmax(row[::-1])] += 1\n",
    "                    #break as soon as we have a state for every such \"i\"\n",
    "                    if not 0 in target_indices:\n",
    "                        break\n",
    "                #we now have (at most) n_state+1 states, each having a different index as optimal action\n",
    "                options = len(down_sample[1])\n",
    "                #we randomly sample over them to balance the training data set with regards to the optimal actions\n",
    "                choice = random.choice(range(options))\n",
    "                #add data to sample dictionary\n",
    "                sample_dict[key] = [[down_sample[0][choice]], [down_sample[1][choice]]]\n",
    "                \n",
    "        #add the selected data of the sample dictionary to the final dictionary\n",
    "        merge_dicts(data_dict,sample_dict)\n",
    "        \n",
    "#print how much time this process took        \n",
    "et = time.time()\n",
    "print(round(et-st), \"seconds to merge sample data into final dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f688e34",
   "metadata": {},
   "source": [
    "We want to see how well our data is balanced in the end with regards to the optimal actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "610bc8ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2 [2538, 2548, 2488, 2426]\n",
      "3 3 [2468, 2517, 2514, 2501]\n",
      "3 4 [2417, 2469, 2524, 2590]\n",
      "4 2 [2067, 2070, 2058, 1995, 1810]\n",
      "4 3 [1973, 2056, 2005, 1935, 2031]\n",
      "4 4 [2104, 1984, 2061, 1831, 2020]\n",
      "5 2 [1863, 1915, 1965, 1896, 1754, 607]\n",
      "5 3 [1727, 1686, 1702, 1739, 1555, 1591]\n",
      "5 4 [1896, 2011, 1924, 1486, 955, 1728]\n",
      "6 2 [1960, 1982, 1971, 1687, 1383, 940, 77]\n",
      "6 3 [2002, 1999, 1898, 1619, 1206, 659, 617]\n",
      "6 4 [2466, 2286, 1870, 1392, 734, 370, 882]\n",
      "7 2 [3155, 2554, 1806, 1153, 680, 427, 223, 2]\n",
      "7 3 [3106, 2588, 1786, 1137, 692, 381, 180, 130]\n",
      "7 4 [3189, 2537, 1708, 1059, 636, 367, 168, 336]\n",
      "8 2 [4376, 2448, 1446, 803, 463, 274, 116, 74, 0]\n",
      "8 3 [3635, 2506, 1708, 958, 590, 321, 167, 81, 34]\n",
      "8 4 [2832, 2512, 1890, 1235, 697, 397, 238, 94, 105]\n"
     ]
    }
   ],
   "source": [
    "#print distribution of optimal actions for every key\n",
    "for key in data_dict:\n",
    "    n_state = key[0]\n",
    "    target_indices = [0]*(n_state+1)\n",
    "    for row in data_dict[key][1]:\n",
    "        target_indices[n_state - np.argmax(row[::-1])] += 1\n",
    "    print(n_state, key[1], target_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b2c415",
   "metadata": {},
   "source": [
    "Lastly, we will transform the data into a form compatible with our Neural Network. Each list of samples is assigned to one key in the final dictionary. These keys are the 2-tuples of the amounts of remaining Jobs and Machines of the associated states. Every list will be saved separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfeb1963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 seconds to transform data\n"
     ]
    }
   ],
   "source": [
    "#transform final dictionary into compatible data-format for an keras LSTM\n",
    "st = time.time()\n",
    "data_into_LSTM_format(data_dict)\n",
    "et = time.time()\n",
    "print(round(et-st), \"seconds to transform data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e749d8",
   "metadata": {},
   "source": [
    "The merged data will be saved in a sub-directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder to save merged LSTM data\n",
    "if DS == \"0\":\n",
    "    LSTM_data_path = f'EstimData/{n}_Jobs/LSTM_EstimData_RR/'\n",
    "    \n",
    "else:\n",
    "    LSTM_data_path = f'DataSet_{DS}/LSTM_Data_RR_{DS}/'\n",
    "    \n",
    "if not os.path.exists(LSTM_data_path):\n",
    "    os.mkdir(LSTM_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save every merged n-m-combination as a pickle file \n",
    "for key in data_dict:\n",
    "    n_state, m_state = key\n",
    "    file_path = f'{LSTM_data_path}{n_state}-jobs-{m_state}-machines'\n",
    "    if DS != \"0\":\n",
    "        file_path += f'_{DS}'\n",
    "    with open(f'{file_path}.pickle', 'wb') as f:\n",
    "            pickle.dump(data_dict[key], f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
