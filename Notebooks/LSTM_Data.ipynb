{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e4e52a",
   "metadata": {},
   "source": [
    "# Bring Data into LSTM-form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec7610",
   "metadata": {},
   "source": [
    "## Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b086d",
   "metadata": {},
   "source": [
    "This notebook applies some technical transformations to given data.<br>\n",
    "The goal is to bring the data into a form that can directly be fed to our Neural Network.<br>\n",
    "So after creating a dictionary of data, this notebook has to be run on int.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2fd08",
   "metadata": {},
   "source": [
    "We will create a final dictionary having each (n,m)-combination as key (with 4<= n <=8 and 2<= m<= 4), that will contain all of the 10.000 data samples. 10.000 was the number we used for every such key.\n",
    "For this, for each of these samples we will:\n",
    "   1. load data sample\n",
    "   2. transform the numpy-arrays into LSTM-form\n",
    "   3. add them to final dictionary\n",
    "    \n",
    "Finally, we will store each of these final dictionary entries as a pickle-list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006c3f1",
   "metadata": {},
   "source": [
    "In the end every Job will be represented as explained in the notebook \"Data_for_NN\" above the function \"seq_data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb2580",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d3ba01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Jobs_and_Machines.ipynb\n",
      "importing Jupyter notebook from States_and_Policies.ipynb\n",
      "importing Jupyter notebook from Global_Variables.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import import_ipynb\n",
    "from Jobs_and_Machines import *\n",
    "from States_and_Policies import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb5b43",
   "metadata": {},
   "source": [
    "### Select Hyperparameters and Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a3117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set key limits\n",
    "n = 8 #max job number\n",
    "m = 4 #max machine number\n",
    "n_min = 3 #min job number\n",
    "m_min = 2 #min machine number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa5134b",
   "metadata": {},
   "source": [
    "Choose the dictionary that shall be transformed. In case that estimated data of higher job numbers, resulting from the approach of applying further techniques of Deep Reinforcement Learning, shall be used, answer <i>0</i> in the following input-question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e22a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Data-Set do you want to work on?\n",
      "Type 0 if you want to work on estimated Data-Sets.\n",
      "01\n"
     ]
    }
   ],
   "source": [
    "#Number of Data Set has to be given in two digits, so for example \"1\" has to beg given as \"01\", while \"98\" stays the same\n",
    "DS = input(\"Which Data-Set do you want to work on?\\n\"+\"Type 0 if you want to work on estimated Data-Sets.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca98126f",
   "metadata": {},
   "source": [
    "In case that the data set is estimated for a higher number of Jobs, this new number has to be stated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ead42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DS == \"0\":\n",
    "    n = int(input(\"What is the new number of Jobs?\\n\"))\n",
    "    n_min = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f6d78",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf8763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to merge dictionaries containing data\n",
    "def merge_dicts(data_dict, sample_dict):\n",
    "    for key in data_dict:\n",
    "        if key in sample_dict:\n",
    "            data_dict[key][0] += sample_dict[key][0] #add inputs\n",
    "            data_dict[key][1] += sample_dict[key][1] #add targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eebbd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring data into format directly readable for Neural Network and its base layer, being an LSTM\n",
    "def data_into_LSTM_format(data_dict):\n",
    "    #iterate over keys\n",
    "    for key in data_dict:\n",
    "        #get inputs and targets\n",
    "        inputs, targets = data_dict[key] #list of list of inputs and list of targets\n",
    "        #inputs is a list, for every state their is one entry, being a list itself \n",
    "        #These inner lists consist of two entries: Job-data and Machine-data of a state\n",
    "        #every machine-data consists of 3 entries, so create indexes for the range of m_state repeating every index 3 times\n",
    "        idxs = [ind+1 for ind in range(key[1]) for _ in range(3)]\n",
    "        \n",
    "        #inputs are now Jobs. Each Job is a sequence of the processing time and the respective machine information.\n",
    "        #the last 2 entries are the jobs earliness and weight\n",
    "        seq_inputs = [np.insert(inp[0],idxs,inp[1].flatten(), axis=1) for inp in inputs]\n",
    "        \n",
    "        #merge samples to numpy array\n",
    "        data_dict[key][0] = [np.stack(seq_inputs)]\n",
    "        data_dict[key][1] = [np.stack(targets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635fdd8a",
   "metadata": {},
   "source": [
    "### Create Final Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddcbb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the final dictionary that will contain the data of all samples\n",
    "#We create an empty dictionary first, that will then get added the sample-data successively\n",
    "data_dict = dict(((n_state,m_state),[[],[]]) \n",
    "                           for n_state in range(n_min,n+1) for m_state in range(m_min,m+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e53a37e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Job-Scheduling-Files'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change working directory\n",
    "os.chdir('D:\\\\Job-Scheduling-Files')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e629b1fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 seconds to merge sample data into final dictionary\n"
     ]
    }
   ],
   "source": [
    "#measure starting time\n",
    "st = time.time()\n",
    "\n",
    "#check if we are using estimated data set\n",
    "if DS == \"0\":\n",
    "    data_path = f'Data/EstimData/{n}_Jobs/estim_data_{n}_Jobs'\n",
    "    data_indices = [str(i+1) for i in range(800)]\n",
    "#else get path to folder of data sets\n",
    "else:\n",
    "    #Directory. We used 10 such folders for the training data\n",
    "    data_path = f'Data/DataSet_{DS}/data_{DS}'\n",
    "    #every file consists of the dictionary of one Job Scheduling Problem\n",
    "    data_indices = [\"0\"*(4-len(str(i))) + str(i) for i in range((int(DS)-1)*10000,int(DS)*10000)] #10000\n",
    "\n",
    "#loop over dictionaries\n",
    "for data_ind in data_indices:\n",
    "    #open dictionary\n",
    "    with open(f'{data_path}_{data_ind}.pickle', 'rb') as f:\n",
    "        #load sample dictionary\n",
    "        sample_dict = pickle.load(f)\n",
    "        #loop over it keys\n",
    "        for key in sample_dict:\n",
    "            #down sample will contain selection of data\n",
    "            down_sample = [[],[]]\n",
    "            n_state = key[0]\n",
    "            #98 denotes the validation data, 99 the test data\n",
    "            if DS in [\"98\", \"99\"]:\n",
    "                #only one state-data per n-m-combination for every Job Scheduling Problem\n",
    "                data_length = 1\n",
    "                #take first data instance\n",
    "                down_sample[0] = sample_dict[key][0][:data_length]\n",
    "                down_sample[1] = sample_dict[key][1][:data_length]\n",
    "                #update current sample dictionary\n",
    "                sample_dict[key] = down_sample\n",
    "            #if training data dictionary\n",
    "            elif int(DS)-1 in range(10):\n",
    "                #target index \"i\" corresponds to \"i\" being the optimal action in a selected state\n",
    "                #we only select the data of on state for every such \"i\" from every dictionary=JobSchedulingProblem\n",
    "                target_indices = [0 for i in range(n_state+1)]\n",
    "                #loop over target-vectors\n",
    "                for i, row in enumerate(sample_dict[key][1]):\n",
    "                    #check if we already saved the data of a state whose optimal action is equal to the one of this target vector\n",
    "                    if target_indices[n_state - np.argmax(row[::-1])] == 0:\n",
    "                        #add input data\n",
    "                        down_sample[0].append(sample_dict[key][0][i])\n",
    "                        #add target values\n",
    "                        down_sample[1].append(row)\n",
    "                        #update that we already have one state with optimal action \"i\" for this dictionary\n",
    "                        target_indices[n_state - np.argmax(row[::-1])] += 1\n",
    "                    #break as soon as we have a state for every such \"i\"\n",
    "                    if not 0 in target_indices:\n",
    "                        break\n",
    "                #we now have (at most) n_state+1 states, each having a different index as optimal action\n",
    "                options = len(down_sample[1])\n",
    "                #we randomly sample over them to balance the training data set with regards to the optimal actions\n",
    "                choice = random.choice(range(options))\n",
    "                #add data to sample dictionary\n",
    "                sample_dict[key] = [[down_sample[0][choice]], [down_sample[1][choice]]]\n",
    "                \n",
    "        #add the selected data of the sample dictionary to the final dictionary\n",
    "        merge_dicts(data_dict,sample_dict)\n",
    "        \n",
    "#print how much time this process took        \n",
    "et = time.time()\n",
    "print(round(et-st), \"seconds to merge sample data into final dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f688e34",
   "metadata": {},
   "source": [
    "We want to see how well our data is balanced in the end with regards to the optimal actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "610bc8ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2 [2530, 2466, 2541, 2463]\n",
      "3 3 [2535, 2459, 2537, 2469]\n",
      "3 4 [2494, 2515, 2504, 2487]\n",
      "4 2 [2107, 2118, 1988, 2007, 1780]\n",
      "4 3 [1999, 2042, 1998, 2018, 1943]\n",
      "4 4 [2062, 2026, 2093, 1781, 2038]\n",
      "5 2 [1848, 1869, 1961, 1931, 1804, 587]\n",
      "5 3 [1740, 1737, 1731, 1744, 1523, 1525]\n",
      "5 4 [1956, 1928, 1820, 1597, 986, 1713]\n",
      "6 2 [2075, 1946, 1911, 1690, 1381, 913, 84]\n",
      "6 3 [1971, 2000, 1944, 1596, 1172, 713, 604]\n",
      "6 4 [2372, 2273, 1944, 1333, 769, 394, 915]\n",
      "7 2 [3099, 2555, 1823, 1165, 742, 397, 216, 3]\n",
      "7 3 [3113, 2519, 1790, 1140, 733, 396, 190, 119]\n",
      "7 4 [3135, 2553, 1695, 1070, 671, 349, 174, 353]\n",
      "8 2 [4376, 2448, 1446, 803, 463, 274, 116, 74, 0]\n",
      "8 3 [3635, 2506, 1708, 958, 590, 321, 167, 81, 34]\n",
      "8 4 [2832, 2512, 1890, 1235, 697, 397, 238, 94, 105]\n"
     ]
    }
   ],
   "source": [
    "#print distribution of optimal actions for every key\n",
    "for key in data_dict:\n",
    "    n_state = key[0]\n",
    "    target_indices = [0]*(n_state+1)\n",
    "    for row in data_dict[key][1]:\n",
    "        target_indices[n_state - np.argmax(row[::-1])] += 1\n",
    "    print(n_state, key[1], target_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfeb1963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 seconds to transform data\n"
     ]
    }
   ],
   "source": [
    "#transform final dictionary into compatible data-format for an keras LSTM\n",
    "st = time.time()\n",
    "data_into_LSTM_format(data_dict)\n",
    "et = time.time()\n",
    "print(round(et-st), \"seconds to transform data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder to save merged LSTM data\n",
    "if DS == \"0\":\n",
    "    LSTM_data_path = f'Data/EstimData/{n}_Jobs/LSTM_EstimData_RR/'\n",
    "    \n",
    "else:\n",
    "    LSTM_data_path = f'Data/DataSet_{DS}/LSTM_Data_RR_{DS}/'\n",
    "    \n",
    "if not os.path.exists(LSTM_data_path):\n",
    "    os.mkdir(LSTM_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save every merged n-m-combination as a pickle file \n",
    "for key in data_dict:\n",
    "    n_state, m_state = key\n",
    "    file_path = f'{LSTM_data_path}{n_state}-jobs-{m_state}-machines'\n",
    "    if DS != \"0\":\n",
    "        file_path += f'_{DS}'\n",
    "    with open(f'{file_path}.pickle', 'wb') as f:\n",
    "            pickle.dump(data_dict[key], f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
