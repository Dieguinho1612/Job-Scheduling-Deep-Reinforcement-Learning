{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3b72ff",
   "metadata": {},
   "source": [
    "# Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49503d",
   "metadata": {},
   "source": [
    "## Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f46321",
   "metadata": {},
   "source": [
    "For a given set of states, corresponding to the decision time points of schedules from a Job Scheduling Problem, we want to extract the information about the states as data that fulfills the markovian requirements and can be fed to a Neural Network.<br> \n",
    "The according code is given within this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67c8e3",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "#import necessary notebooks\n",
    "import import_ipynb\n",
    "#from Jobs_and_Machines import *\n",
    "#from States_and_Policies import *\n",
    "from Global_Variables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae2a18",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e121d",
   "metadata": {},
   "source": [
    "First, we create the Input. For any state, it will consist of the remaining Jobs and Machines. We do not consider past Jobs or Machines due to the markovian requirements. For the same reason, we do not pass explicitely pass the time, but instead give this information implicitely by defining the earliness for every remaining Job and Machine as the time until its deadline exceeds, i.e. as the maximum of zero and its deadline minus the current time.<br>\n",
    "For every pending Job we will pass the information of its processing times on the working Machines, its earliness and its weight. They get sorted by the shortest processing time on the currently free Machine.<br>\n",
    "For every working Machine we will pass its remaining occupation time, its earliness and its weight. They get sorted by the current occupation time, thus putting the currently free Machine in first place.<br>\n",
    "In case that any two Jobs or Machines are equal with regard to the respective rule stated above, they are already sorted for their weight and deadlines due to construction.<br>\n",
    "Moreover, the input gets normalized. Every weight gets normalized statically, being divided by the maximum allowed weight. The Job processing times, Machine occupation times and all earlinesses get normalized dynamically with regard to their state, getting scaled by the greatest time-related value occuring in said state. The respective permutation of Jobs and Machines becomes a state attribute as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffebe7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take information of state to create normalized data for Neural Network\n",
    "def state_input(state):\n",
    "    \n",
    "    #information about the jobs (normalized in weight)\n",
    "    jobs_data = np.asarray([[proc_time for i, proc_time in enumerate(job.processing_time)\n",
    "                             if state.machines_on_duty[i] == 1]\n",
    "                            + [max(job.deadline-state.time,0),\n",
    "                               job.weight/max_weight]\n",
    "                            for job in list_jobs if state.jobs_remaining[job.index] == 1], dtype=np.float32)\n",
    "    \n",
    "    #information about the machines (normalized in weight)\n",
    "    machines_data = np.asarray([[state.machine_runtimes[machine.index],\n",
    "                                 max(machine.deadline-state.time,0),\n",
    "                                 machine.weight/max_weight]\n",
    "                                for machine in list_machines if state.machines_on_duty[machine.index] == 1], dtype=np.float32)\n",
    "    \n",
    "    #we need to know the time-related value to scale the data\n",
    "    max_time = max(np.max(jobs_data[:,:-1]), np.max(machines_data[:,:-1])) #:-1 because the last column is the weight\n",
    "    \n",
    "    #normlalize time data\n",
    "    jobs_data[:,:-1] /= max_time\n",
    "    machines_data[:,:-1] /= max_time #for when target values get normalized, too\n",
    "    \n",
    "    #sort them and save permutation\n",
    "    machines_perm = machines_data[:,0].argsort() #sort machines by remaining runtime\n",
    "    orig_order = np.arange(len(machines_perm)) #just an array of the form [0,1,...,m_state-1]\n",
    "    jobs_data[:,orig_order] = jobs_data[:,machines_perm] #reorder processing time of jobs by new order of machines\n",
    "    jobs_perm = jobs_data[:,0].argsort() #order of jobs by processing time for currently free machine\n",
    "    jobs_data = jobs_data[jobs_perm] #sort jobs by this order\n",
    "    machines_data = machines_data[machines_perm] #sort machines by their remaining runtime\n",
    "    \n",
    "    #merge\n",
    "    state.input = [jobs_data, machines_data]\n",
    "    state.permutation = [jobs_perm,machines_perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f31aa",
   "metadata": {},
   "source": [
    "We also give a function that will be used in later Notebooks that merges the information about the Jobs and Machines.<br>\n",
    "For every Job, the processing times get split from the earliness and weight. Then, for every processing time, the entire information of the occupation time, earliness and weight of the associated Machine gets added.<br>\n",
    "As a result, every Job then consists of\n",
    "- a vector of dimension <i>2</i>, consisting of the normalized earliness and weight\n",
    "- a sequence of dimension <i>(4 x m_state)</i>, each vector of dimension <i>4</i> consisting of the processing time and the associated Machine information \n",
    "\n",
    "where <i>m_state</i> stands for the number of working Machines in the respective state. This sequence can then later be fed into an LSTM to create an embedded representation of static dimension for each Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input data of state in sequential form (for LSTM)\n",
    "def seq_data(state):\n",
    "    \n",
    "    #create the input\n",
    "    state_input(state)\n",
    "    #number of jobs and machines in given state\n",
    "    n_state = sum(state.jobs_remaining)\n",
    "    m_state = sum(state.machines_on_duty)\n",
    "    \n",
    "    #indices where the respective machine information gets added to processing time\n",
    "    idxs = [ind+1 for ind in range(m_state) for _ in range(3)] #for every Machine, three consecutive indices are needed\n",
    "    #add the respective machine information to processing time\n",
    "    data = np.insert(state.input[0],idxs,state.input[1].flatten(), axis=1) #info gets inserted into vector of single processing time\n",
    "    \n",
    "    #For every job, the machine environment is the time-series of resources successively becoming available \n",
    "    resource_info = data[:,:-2].reshape((n_state,m_state,4))\n",
    "    #For every job, its earliness and weight denotes its urgency\n",
    "    urgency_info = data[:,-2:]\n",
    "    \n",
    "    state.input = [resource_info, urgency_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915dc072",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953b020",
   "metadata": {},
   "source": [
    "Next, we create the Target Values for every state. These correspond to the Q-values. So if there are <i>n_state</i> pending jobs in a state <i>s</i>, the <i>j</i>-th entry denotes the Q-values <i>Q<sup>*</sup>(s,j)</i> of assigning Job <i>j</i> to the currently free Machine, while the <i>n_state+1</i>-th entry stands for the Q-value of the action of deactivating the same.<br>\n",
    "The Target Values then get dynamically normalized with regards to their state as well. For this, they get divided by the smallest (and therefore optimal) Q-value. Then we take the invers of the resulting quotient. Consequently, all Target Values are scaled inbetween 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create normalized target for states as data for Neural Network\n",
    "def state_target(state):\n",
    "    \n",
    "    #number of remaining jobs\n",
    "    n_state = sum(state.jobs_remaining)\n",
    "    #get Qvalues of all feasible actions\n",
    "    target = np.array([qvalue for qvalue in state.Qvalues if qvalue != None], dtype=np.float32)\n",
    "    #sort by permutation\n",
    "    target[np.arange(n_state)] = target[state.permutation[0]]\n",
    "    #in case that different NN approaches shall be tested, we save the targets in different forms\n",
    "    state.target = [target, #raw Q-values\n",
    "                    np.eye(target.shape[0], dtype=np.float32)[np.argmin(target)], #one_hot_vector of optimal action\n",
    "                   np.min(target)/target] #normalized targets by scaling through minimum Q-value and then taking inverse value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a86077",
   "metadata": {},
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4846cc",
   "metadata": {},
   "source": [
    "We now state a function that creates a dictionary for the data of the states of a given Job Scheduling Problem.<br>\n",
    "The keys are the 2-tuples (<i>n_state</i>,<i>m_state</i>) of the number of remaining Jobs <i>n_state</i> and Machines <i>m_state</i> of each state.<br>\n",
    "To avoid unnecessary comupational costs and to balance the training data set, we do not want to create the data for every state. Instead, for every (<i>n_state</i>,<i>m_state</i>)-combination (i.e. for every key) we have an upper limit of states, denoted by the variable <i>data_points_max</i>. From these we then select at most <i>data_points_max/(n_state+1)</i> states where action <i>a</i> is optimal for every feasible action <i>1 &leq; a &leq; n_state+1</i>.<br>\n",
    "\n",
    "We do not wish to balance the validation and test set. To create them, set <i>training=False</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d93fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data\n",
    "def create_data(all_states, data_points_max, training=True, save=False):\n",
    "    \n",
    "    #measure start time\n",
    "    st = time.time()\n",
    "    \n",
    "    #the minimum amount of jobs and machines a state has to have for us to be interesting enough to save its data\n",
    "    n_min = 3\n",
    "    m_min = 2\n",
    "    \n",
    "    #data will be a tuple consisting of inputs list and targets list\n",
    "    data_dictionary = dict(((n_state,m_state),([],[])) \n",
    "                           for n_state in range(n_min,n+1) for m_state in range(m_min,m+1))\n",
    "    #counter of how many data points there are already for each job i to be the optimal action (+option of machine shut down)\n",
    "    data_points_counter = dict(((n_state,m_state,i),0) \n",
    "                               for n_state in range(n_min,n+1) for m_state in range(m_min,m+1) for i in range(n_state+1))\n",
    "    \n",
    "    #permutations that will be added in data below\n",
    "    #we add the max_runtime+1 as last entry, so that \"n\" (=turning off machine) is always the last entry of permutation\n",
    "    permutations = [np.argsort([job.processing_time[i] for job in list_jobs]+[max_runtime+1]) for i in range(m)]\n",
    "    \n",
    "    #create data for states\n",
    "    for state in all_states:\n",
    "        #key\n",
    "        n_state = sum(state.jobs_remaining)\n",
    "        m_state = sum(state.machines_on_duty)\n",
    "        if n_state >= n_min and m_state >= m_min:\n",
    "            #find out which of the n_state jobs + machine shut down is best action\n",
    "            rev_perm_target = np.array([qvalue for qvalue in np.array(state.Qvalues)[permutations[state.machine]][::-1] if qvalue != None])\n",
    "            #reversed for emphasis on higher indices equality cases\n",
    "            opt_action = len(rev_perm_target) - np.argmin(rev_perm_target) - 1\n",
    "            #use this condition to create balanced training data\n",
    "            if training == True:\n",
    "                cond = data_points_counter[(n_state,m_state,opt_action)]\n",
    "                upper_lim = data_points_max/len(rev_perm_target)\n",
    "            #use this condition instead if you want to create test/validation data without balancing\n",
    "            else:\n",
    "                cond = len(data_dictionary[(n_state,m_state)][1])\n",
    "                upper_lim = data_points_max/len(rev_perm_target)\n",
    "            #check if we have enough data of states already. \n",
    "            if cond < upper_lim:\n",
    "                #create input\n",
    "                state_input(state)\n",
    "                #add input to dictionary\n",
    "                data_dictionary[(n_state,m_state)][0].append(state.input)\n",
    "                #create target values\n",
    "                state_target(state)\n",
    "                #add target values to dictionary\n",
    "                data_dictionary[(n_state,m_state)][1].append(state.target[2])\n",
    "                #update counter\n",
    "                data_points_counter[(n_state,m_state,opt_action)] += 1\n",
    "                \n",
    "    #measure end time\n",
    "    et = time.time()\n",
    "    \n",
    "    #tell how much time the entire process took\n",
    "    print(round(et-st,2), \"seconds to compute\", sum(len(data_dictionary[key][0]) for key in data_dictionary), \"data points.\")\n",
    "    \n",
    "    #if desired, save the data dictionary\n",
    "    if save:\n",
    "        with open('data.pickle', 'wb') as f:\n",
    "            pickle.dump(data_dictionary, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2acfc83",
   "metadata": {},
   "source": [
    "To store the produced dictionary of data, we give the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store data dictionary\n",
    "def store_data(all_states, data_points_max, DS, DN, training=True):\n",
    "    \n",
    "    #create data dictionary\n",
    "    data = create_data(all_states, data_points_max, training=True) #use training=False for validation or test data\n",
    "    \n",
    "    #give indices to Job Scheduling Problem. DS stands for Data Set, DN for Data Number\n",
    "    DS_str = \"0\"*(2-len(str(DS))) + str(DS)\n",
    "    DN_str = \"0\"*(4-len(str(DN))) + str(DN)\n",
    "    path = f'Data/DataSet_{DS_str}/data_{DS_str}_{DN_str}.pickle'\n",
    "    with open(path, 'wb') as f:\n",
    "            pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
